<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Tracking and segmentation on mobile phone captured RGB-D dataset.">
  <meta name="keywords" content="RGB-D Datasets, Tracking and Segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->
  <link rel="icon" href="./static/images/iiaulab.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=rk1ozXMAAAAJ">Haojie Zhao</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a>Junsong Chen</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="http://faculty.dlut.edu.cn/wanglj/zh_CN/index.htm">Lijun Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=D3nE0agAAAAJ">Huchuan Lu</a><sup>1,2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Dalian University of Technology, China,</span>
            <span class="author-block"><sup>2</sup>Peng Cheng Laboratory, China</span>
          </div>
          <nobr>(* indicates equal contributions)</nobr>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://github.com/lawrence-cj/ARKitTrack"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/lawrence-cj/ARKitTrack"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/lawrence-cj/ARKitTrack"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/lawrence-cj/ARKitTrack"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <div class="center-video">
      <iframe width="900" height="500" src="https://www.youtube.com/embed/r02f6egcpdw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>        </iframe>
      </div>

      <h2 class="subtitle has-text-centered">
        ARKitTrack performs on VOT test set.
      </h2>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <div class="center-video">
      <iframe width="900" height="500" src="https://www.youtube.com/embed/bQVnaxB4frw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>      </div>

      <h2 class="subtitle has-text-centered">
        ARKitTrack performs on VOS test set.
      </h2>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure>
        <img src="./static/images/data_vis.jpg" alt="light_and_fark"  style="width:100%">
      </figure>
      <h2 class="subtitle has-text-centered">
        We propose ARKitTrack to significantly facilitate RGB-D tracking and segmentation.
      </h2>
      <div class="content has-text-justified">
        <p>
        <b>Figure 1. Samples from <span class="dnerf">ARKitTrack</span>.</b> We capture both indoor and outdoor sequences (1st row) in many scenes,
        including zoo, market, office, square, corridor, etc. Lots of scenarios are presented in our dataset,
        e.g., low or high light conditions (2nd row), surrounding clutter (3rd row), out-of-plane rotation,
        motion blur, deformation, etc. (4th row). Besides, we annotate each frame with object masks.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Compared with traditional RGB-only visual tracking, few datasets have been constructed for RGB-D tracking.
            In this paper, we propose ARKitTrack, a new RGB-D track-ing dataset for both static and dynamic scenes captured
            by consumer-grade LiDAR scanners equipped on Apple’s iPhone and iPad.
          </p>
          <p>
             <span class="dnerf">ARKitTrack</span> contains 300 RGB-D sequences, 455 targets, and 229.7K video frames in total.
            Along with the bounding box annotations and frame-level attributes,
            we also annotate this dataset with 123.9K pixel-level target masks. Besides,
            the camera intrinsic and camera pose of each frame are provided for future developments.
            To demonstrate the potential usefulness of this dataset,
            we further present a unified baseline for both box-level and pixel-level tracking,
            which integrates RGB features with bird’s-eye-view representations to better explore cross-modality 3D geometry.
          </p>
          <p>
            In-depth empirical analysis has verified that the <span class="dnerf">ARKitTrack</span> dataset
            can significantly facilitate RGB-D tracking and that
            the proposed baseline method compares favorably against the state of the arts.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
      <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Pipeline</h2>
        </br>
        <img src="./static/images/pipeline.jpg" class="interpolation-image"
         alt="Interpolate start reference image." />
        </br>
        </br>
        <div class="content has-text-justified">
          <p>
            <b>Figure 2. Overview the proposed unified RGB-D tracking pipeline in both box-level (VOT) and pixel-level (VOS).</b>
          </p>
        </div>
        <div class="content has-text-justified">
          <p>
            The procedure of our method is as follows.
            We first use the ViT model to extract image feature map <b>I</b>,
            which is further projected into the 3D space to generate the BEV feature map  <b>B</b>
            according to the input depth, pixel coordinate and camera intrinsic.
            The BEV feature is processed in the pillar format using BEV pooling and conv layers.
            The processed BEV feature is back-projected to the 2D image space, producing  <b>I_BEV</b>.
            The image feature  <b>I</b> and BEV feature <b>I_BEV</b> are fused via
            concatenation and conv layers to produce the final feature map.
            The image-BEV cross-view (re-)projection mainly follows LSS.
            Our framework can be applied to both RGBD VOT and VOS, yielding superior performance even with naive output heads.
            More details could be found in our CVPR2023 paper.
          </p>
        </div>
      </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
      <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
        </div>
        </br>
        <img src="./static/images/vot_results.jpg" class="interpolation-image"
         alt="Interpolate start reference image." />
        </br>
        </br>
        <div class="content has-text-justified">
          <p>
            <b>Table 1. Comparisons with SoTA VOT methods on ARKitTrack test set and existing popular RGB-D benchmarks, including DepthTrack and CDTB. </b>
            For all trackers, the overall performance on our dataset is always lower than that on DepthTrack and CDTB.
            It confirms that the proposed ARKitTrack is more challenging than the existing RGB-D tracking datasets.
            Our tracker performs better than other trackers on both ARKitTrack and DepthTrack,
            achieving 0.478 and 0.612 F-score, respectively.
            Although CDTB does not provide a training set and our tracker cannot gain from model learning on this dataset,
            our method still achieves satisfactory performance. 0.677 F-score can be achieved by training with ARKitTrack,
            and 0.690 F-score is achieved by training with DepthTrack.
          </p>
        </div>

        <div class="content has-text-justified">
        </div>
        </br>
        <img src="static/images/vos_results.jpg" class="interpolation-image"
         alt="Interpolate start reference image." />
        </br>
        </br>
        <div class="content has-text-justified">
          <p>
            <b>Table 2. Comparisons with SoTA VOS methods on ARKitTrack test set.</b>
            Since there is no existing RGB-D VOS method and dataset, we select 4 state-of-the-art
            RGB VOS methods for comparison on the ARKitTrack-VOS test set.
            Besides, We design a variant named STCN_RGBD for RGB-D VOS by adding
            an additional depth branch to STCN and fusing RGBD features through concatenation.
            For a fair comparison, all methods are retrained on ARKitTrack-VOS train set without static image pre-training.
          </p>
        </div>

        <div class="content has-text-justified">
        </div>
        </br>
        <img src="static/images/vot_attribute_analysis.jpg" class="interpolation-image"
         alt="Interpolate start reference image."/>
        </br>
        </br>
        <div class="content has-text-justified">
          <p>
            <b>Figure 3. Attribute-specific analysis on the VOT test set.</b>
            We also conduct an attribute-specific analysis for the aforementioned RGB-D VOT trackers
            by using our per-frame attributes annotation.
            It shows that our tracker performs better than other RGB-D trackers on 10 attributes.
            Besides, DeT outperforms other trackers on the full-occlusion attribute.
            All trackers can deliver satisfactory performance on the extreme-illumination factor.
            However, none of them can well address the fast-motion and out-of-view attributes,
            indicating these attributes are more challenging for existing RGB and RGB-D trackers.
          </p>
        </div>

        <div class="content has-text-justified">
        </div>
        </br>
        <img src="static/images/vot_abalations.jpg" class="interpolation-image"
         alt="Interpolate start reference image." />
        </br>
        </br>
        <div class="content has-text-justified">
          <p>
            <b>Table 3. The ablation analysis of our VOT method on the ARKitTrack test set and DepthTrack test set.</b>
            Our tracker is trained with their respective training sets and tested on their respective test sets.
            The basic tracker is OSTrack, which is trained with only the common RGB tracking datasets.
            <b>FT, BEV, CV, GD</b> stands for Fine-tuning, BEV space, Cross views and Gaussian distribution.
            More details could be found in our paper.
          </p>
        </div>

        <div class="content has-text-justified">
        </div>
        </br>
        <img src="static/images/vos_abalations.jpg" class="interpolation-image"
         alt="Interpolate start reference image." />
        </br>
        </br>
        <div class="content has-text-justified">
          <p>
            <b>Table 4. Comparison with different memorizing strategies.</b>
            We also explore different memory update strategies for VOS.
            <b>ONE:</b> Keep only one template and update it every N frames.
            <b>IP:</b> Using the IoU prediction branch to control the template update.
            <b>ADD:</b> Consecutively add new templates to the memory bank and maintain several templates for prediction.
            More details could be found in our paper.
          </p>
        </div>


      </div>
      </div>


    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhao2023arkittrack,
  author    = {Haojie Zhao and Junsong Chen and Lijun Wang and Huchuan Lu},
  title     = {ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data},
  journal   = {CVPR},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/images/preprint.jpg">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/lawrence-cj" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The website template was borrowed from <a href="https://nerfies.github.io/">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
